----------------------------------------
Starting experiment resnet_12-1559491145
Experiment parameters Experiment[name: resnet_12-1559491145, model: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1, bias=True)
), params: Params(lr: 0.0001, weight_decay: 0, batch_size: 32, num_epochs: 20), optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
), criterion: BCEWithLogitsLoss()]
start metrics
eval metrics acc, f1
0.500457763671875, 0.0
train metrics acc, f1
0.5, 0.0
Epoch 1/20
----------
eval metrics, batch: 1024 acc, f1
0.75494384765625, 0.7055804062477085
eval metrics, batch: 2048 acc, f1
0.804656982421875, 0.8217041308041558
eval metrics, batch: 3072 acc, f1
0.8089599609375, 0.820228591120556
eval metrics, batch: 4096 acc, f1
0.807952880859375, 0.8014137397835212
train metrics, batch: 4096  acc, f1 
0.8556747436523438, 0.8582401606666467
eval metrics, batch: 5120 acc, f1
0.7867431640625, 0.7657862984314251
eval metrics, batch: 6144 acc, f1
0.804656982421875, 0.794305729618561
eval metrics, batch: 7168 acc, f1
0.754547119140625, 0.7053306466385785
Epoch loss - train: tensor(0.3670, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4194, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.8189697265625, 0.8107090433339715
train metrics acc, f1 
0.8839874267578125, 0.8861996707079779
Epoch 2/20
----------
eval metrics, batch: 1024 acc, f1
0.7811279296875, 0.7664452259997395
eval metrics, batch: 2048 acc, f1
0.81915283203125, 0.8078594124894625
eval metrics, batch: 3072 acc, f1
0.82501220703125, 0.8132247557003257
eval metrics, batch: 4096 acc, f1
0.772491455078125, 0.7295483402865953
train metrics, batch: 4096  acc, f1 
0.8867301940917969, 0.880231043203627
eval metrics, batch: 5120 acc, f1
0.82135009765625, 0.8168220789786594
eval metrics, batch: 6144 acc, f1
0.771820068359375, 0.7265679283232767
eval metrics, batch: 7168 acc, f1
0.8134765625, 0.8044410315479619
Epoch loss - train: tensor(0.2863, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.3853, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.827117919921875, 0.8322227158300015
train metrics acc, f1 
0.88201904296875, 0.8907330860272037
Epoch 3/20
----------
eval metrics, batch: 1024 acc, f1
0.785797119140625, 0.7535030728709394
eval metrics, batch: 2048 acc, f1
0.78741455078125, 0.747773191396915
eval metrics, batch: 3072 acc, f1
0.7666015625, 0.7207128250073035
eval metrics, batch: 4096 acc, f1
0.795196533203125, 0.7694527465732248
train metrics, batch: 4096  acc, f1 
0.9135513305664062, 0.9130838325649896
eval metrics, batch: 5120 acc, f1
0.7911376953125, 0.757700205338809
eval metrics, batch: 6144 acc, f1
0.795318603515625, 0.7625420428394406
eval metrics, batch: 7168 acc, f1
0.83807373046875, 0.834011136832885
Epoch loss - train: tensor(0.2371, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4532, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.807281494140625, 0.7798500958689211
train metrics acc, f1 
0.9271888732910156, 0.9259582523556269
Epoch 4/20
----------
eval metrics, batch: 1024 acc, f1
0.81298828125, 0.7958694203864091
eval metrics, batch: 2048 acc, f1
0.817169189453125, 0.8124236826450422
eval metrics, batch: 3072 acc, f1
0.823486328125, 0.8046077967704884
eval metrics, batch: 4096 acc, f1
0.75347900390625, 0.6884929816443005
train metrics, batch: 4096  acc, f1 
0.918212890625, 0.9133366209366436
eval metrics, batch: 5120 acc, f1
0.76678466796875, 0.7100910470409711
eval metrics, batch: 6144 acc, f1
0.7828369140625, 0.7404814004376368
eval metrics, batch: 7168 acc, f1
0.78759765625, 0.745985401459854
Epoch loss - train: tensor(0.2054, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.7254, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.727752685546875, 0.6356544823361242
train metrics acc, f1 
0.8961715698242188, 0.8858391564395306
Epoch 5/20
----------
eval metrics, batch: 1024 acc, f1
0.7835693359375, 0.7454780361757106
eval metrics, batch: 2048 acc, f1
0.768310546875, 0.7148223273983924
eval metrics, batch: 3072 acc, f1
0.833160400390625, 0.8213223518645619
eval metrics, batch: 4096 acc, f1
0.802734375, 0.7727784026996626
train metrics, batch: 4096  acc, f1 
0.9456405639648438, 0.9450922065611923
eval metrics, batch: 5120 acc, f1
0.78057861328125, 0.733999260081391
eval metrics, batch: 6144 acc, f1
0.7755126953125, 0.7267053053945609
eval metrics, batch: 7168 acc, f1
0.80718994140625, 0.7867701653729329
Epoch loss - train: tensor(0.1821, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.5385, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.79376220703125, 0.7578645646721606
train metrics acc, f1 
0.94696044921875, 0.9459682120234718
Epoch 6/20
----------
eval metrics, batch: 1024 acc, f1
0.804718017578125, 0.7745163677367067
eval metrics, batch: 2048 acc, f1
0.811187744140625, 0.7843273956844564
eval metrics, batch: 3072 acc, f1
0.840087890625, 0.8291712851274695
eval metrics, batch: 4096 acc, f1
0.824981689453125, 0.8196143805240147
train metrics, batch: 4096  acc, f1 
0.9281883239746094, 0.931493888126699
eval metrics, batch: 5120 acc, f1
0.79742431640625, 0.7616345877621373
eval metrics, batch: 6144 acc, f1
0.789825439453125, 0.7493540051679587
eval metrics, batch: 7168 acc, f1
0.786346435546875, 0.7421077835488268
Epoch loss - train: tensor(0.1595, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4233, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.828460693359375, 0.8157351253892804
train metrics acc, f1 
0.9497795104980469, 0.95062983060763
Epoch 7/20
----------
eval metrics, batch: 1024 acc, f1
0.791412353515625, 0.7569432097009352
eval metrics, batch: 2048 acc, f1
0.7923583984375, 0.7591163350562912
eval metrics, batch: 3072 acc, f1
0.827178955078125, 0.8153630465260344
eval metrics, batch: 4096 acc, f1
0.75592041015625, 0.6939384662482779
train metrics, batch: 4096  acc, f1 
0.9474754333496094, 0.9455506036531595
eval metrics, batch: 5120 acc, f1
0.804046630859375, 0.7718762212669201
eval metrics, batch: 6144 acc, f1
0.800933837890625, 0.7717395107953949
eval metrics, batch: 7168 acc, f1
0.82928466796875, 0.8157808074820523
Epoch loss - train: tensor(0.1384, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4396, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.832061767578125, 0.8233556960806343
train metrics acc, f1 
0.9515838623046875, 0.9529221501802697
Epoch 8/20
----------
eval metrics, batch: 1024 acc, f1
0.794281005859375, 0.758689815643458
eval metrics, batch: 2048 acc, f1
0.8070068359375, 0.7812067533905341
eval metrics, batch: 3072 acc, f1
0.803497314453125, 0.7711716834286932
eval metrics, batch: 4096 acc, f1
0.79541015625, 0.7611685073031706
train metrics, batch: 4096  acc, f1 
0.9661788940429688, 0.9658498255128689
eval metrics, batch: 5120 acc, f1
0.820587158203125, 0.8015661389948358
eval metrics, batch: 6144 acc, f1
0.775604248046875, 0.7265627905247108
eval metrics, batch: 7168 acc, f1
0.7718505859375, 0.7256312389900176
Epoch loss - train: tensor(0.1214, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4774, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.819061279296875, 0.8036819972848581
train metrics acc, f1 
0.9604263305664062, 0.9611466494883972
Epoch 9/20
----------
eval metrics, batch: 1024 acc, f1
0.806640625, 0.7830582756967747
eval metrics, batch: 2048 acc, f1
0.783660888671875, 0.7368694554767826
eval metrics, batch: 3072 acc, f1
0.80523681640625, 0.783675683004542
eval metrics, batch: 4096 acc, f1
0.79339599609375, 0.7552068267283772
train metrics, batch: 4096  acc, f1 
0.9666366577148438, 0.9660797393732548
eval metrics, batch: 5120 acc, f1
0.796112060546875, 0.7636633768438926
eval metrics, batch: 6144 acc, f1
0.79498291015625, 0.7563116656993616
eval metrics, batch: 7168 acc, f1
0.81768798828125, 0.8040283427371736
Epoch loss - train: tensor(0.1062, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.5546, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.8184814453125, 0.7949813870122708
train metrics acc, f1 
0.9735145568847656, 0.9734849208137453
Epoch 10/20
----------
eval metrics, batch: 1024 acc, f1
0.79156494140625, 0.7529837251356238
eval metrics, batch: 2048 acc, f1
0.804290771484375, 0.7802487749717301
eval metrics, batch: 3072 acc, f1
0.803863525390625, 0.7797992256826669
eval metrics, batch: 4096 acc, f1
0.79962158203125, 0.7664674918196045
train metrics, batch: 4096  acc, f1 
0.9778938293457031, 0.9777651586366723
eval metrics, batch: 5120 acc, f1
0.778228759765625, 0.7355435059499982
eval metrics, batch: 6144 acc, f1
0.772857666015625, 0.7262898540065458
eval metrics, batch: 7168 acc, f1
0.790496826171875, 0.7557721726137536
Epoch loss - train: tensor(0.0916, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.6564, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.797088623046875, 0.763641534250471
train metrics acc, f1 
0.9763755798339844, 0.9761804943903198
Epoch 11/20
----------
eval metrics, batch: 1024 acc, f1
0.79937744140625, 0.7703486341088521
eval metrics, batch: 2048 acc, f1
0.80828857421875, 0.7828101230811783
eval metrics, batch: 3072 acc, f1
0.789154052734375, 0.75276435856146
eval metrics, batch: 4096 acc, f1
0.823822021484375, 0.8095410906931477
train metrics, batch: 4096  acc, f1 
0.9678497314453125, 0.9684034520765695
eval metrics, batch: 5120 acc, f1
0.809967041015625, 0.7834764769289614
eval metrics, batch: 6144 acc, f1
0.8011474609375, 0.7775198033324229
eval metrics, batch: 7168 acc, f1
0.798431396484375, 0.7640650116092159
Epoch loss - train: tensor(0.0797, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.6501, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.814117431640625, 0.7949503450597543
train metrics acc, f1 
0.9799728393554688, 0.9801428204003207
Epoch 12/20
----------
eval metrics, batch: 1024 acc, f1
0.805694580078125, 0.7801526190393978
eval metrics, batch: 2048 acc, f1
0.7882080078125, 0.7550300035298271
eval metrics, batch: 3072 acc, f1
0.803009033203125, 0.7741822634248732
eval metrics, batch: 4096 acc, f1
0.796661376953125, 0.7655111736758754
train metrics, batch: 4096  acc, f1 
0.9776687622070312, 0.9775762079506017
eval metrics, batch: 5120 acc, f1
0.79193115234375, 0.7594892055876957
eval metrics, batch: 6144 acc, f1
0.776763916015625, 0.7301435053676172
eval metrics, batch: 7168 acc, f1
0.80755615234375, 0.7816028260718986
Epoch loss - train: tensor(0.0695, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.8105, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.78277587890625, 0.7407299482771181
train metrics acc, f1 
0.9789657592773438, 0.9786873840445269
Epoch 13/20
----------
eval metrics, batch: 1024 acc, f1
0.80596923828125, 0.7739779594738713
eval metrics, batch: 2048 acc, f1
0.805694580078125, 0.7774787683919897
eval metrics, batch: 3072 acc, f1
0.773895263671875, 0.721246096542383
eval metrics, batch: 4096 acc, f1
0.812652587890625, 0.7875263904752016
train metrics, batch: 4096  acc, f1 
0.984619140625, 0.9846240676052901
eval metrics, batch: 5120 acc, f1
0.769805908203125, 0.7157553604401402
eval metrics, batch: 6144 acc, f1
0.795166015625, 0.7629273806159932
eval metrics, batch: 7168 acc, f1
0.761199951171875, 0.7008220225578283
Epoch loss - train: tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.8809, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.799102783203125, 0.7666182153366186
train metrics acc, f1 
0.9850845336914062, 0.9849970838321516
Epoch 14/20
----------
eval metrics, batch: 1024 acc, f1
0.802734375, 0.769554367201426
eval metrics, batch: 2048 acc, f1
0.828369140625, 0.8151945320715037
eval metrics, batch: 3072 acc, f1
0.795013427734375, 0.7617324678088752
eval metrics, batch: 4096 acc, f1
0.819122314453125, 0.8013007475946227
train metrics, batch: 4096  acc, f1 
0.9872283935546875, 0.9873066424021838
eval metrics, batch: 5120 acc, f1
0.795623779296875, 0.7618675105785301
eval metrics, batch: 6144 acc, f1
0.805694580078125, 0.7834869248818308
eval metrics, batch: 7168 acc, f1
0.804840087890625, 0.775353918572382
Epoch loss - train: tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.7368, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.818756103515625, 0.7990798064887175
train metrics acc, f1 
0.9851837158203125, 0.9852647712700978
Epoch 15/20
----------
eval metrics, batch: 1024 acc, f1
0.818603515625, 0.7983444157959018
eval metrics, batch: 2048 acc, f1
0.814849853515625, 0.7960466601674119
eval metrics, batch: 3072 acc, f1
0.814178466796875, 0.79140831078072
eval metrics, batch: 4096 acc, f1
0.77618408203125, 0.7276643148904567
train metrics, batch: 4096  acc, f1 
0.9860877990722656, 0.9859316836075376
eval metrics, batch: 5120 acc, f1
0.79425048828125, 0.7595749233292918
eval metrics, batch: 6144 acc, f1
0.806549072265625, 0.7785192690681667
eval metrics, batch: 7168 acc, f1
0.797271728515625, 0.7648412333179936
Epoch loss - train: tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(1.2563, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.742889404296875, 0.6683984728618098
train metrics acc, f1 
0.9774360656738281, 0.9769477495313553
Epoch 16/20
----------
eval metrics, batch: 1024 acc, f1
0.81591796875, 0.79125138427464
eval metrics, batch: 2048 acc, f1
0.8046875, 0.7774222716839396
eval metrics, batch: 3072 acc, f1
0.816925048828125, 0.7987115391068014
eval metrics, batch: 4096 acc, f1
0.8079833984375, 0.78665400786654
train metrics, batch: 4096  acc, f1 
0.9884109497070312, 0.9884910292156625
eval metrics, batch: 5120 acc, f1
0.802215576171875, 0.7710298533827945
eval metrics, batch: 6144 acc, f1
0.7991943359375, 0.76415770609319
eval metrics, batch: 7168 acc, f1
0.793914794921875, 0.7581217092302733
Epoch loss - train: tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.7651, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.825164794921875, 0.8123423629991156
train metrics acc, f1 
0.9873046875, 0.9874102488443002
Epoch 17/20
----------
eval metrics, batch: 1024 acc, f1
0.815643310546875, 0.7927260250471779
eval metrics, batch: 2048 acc, f1
0.807769775390625, 0.7817016115058049
eval metrics, batch: 3072 acc, f1
0.811676025390625, 0.7872582480091013
eval metrics, batch: 4096 acc, f1
0.7972412109375, 0.7693055555555556
train metrics, batch: 4096  acc, f1 
0.9865188598632812, 0.986564576711933
eval metrics, batch: 5120 acc, f1
0.789337158203125, 0.7498278548907331
eval metrics, batch: 6144 acc, f1
0.808837890625, 0.7820155902004454
eval metrics, batch: 7168 acc, f1
0.817230224609375, 0.7956879200354792
Epoch loss - train: tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.9273, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.80633544921875, 0.7783908367090375
train metrics acc, f1 
0.9937744140625, 0.9937765032490314
Epoch 18/20
----------
eval metrics, batch: 1024 acc, f1
0.797210693359375, 0.7642028316951137
eval metrics, batch: 2048 acc, f1
0.79925537109375, 0.7647689886997568
eval metrics, batch: 3072 acc, f1
0.79791259765625, 0.7648604502521128
eval metrics, batch: 4096 acc, f1
0.82427978515625, 0.8105173094642623
train metrics, batch: 4096  acc, f1 
0.9880104064941406, 0.9881102347311279
eval metrics, batch: 5120 acc, f1
0.80389404296875, 0.7774776646582173
eval metrics, batch: 6144 acc, f1
0.8033447265625, 0.7728266234224072
eval metrics, batch: 7168 acc, f1
0.806884765625, 0.7792352777002511
Epoch loss - train: tensor(0.0357, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.9488, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.823211669921875, 0.8049297908879685
train metrics acc, f1 
0.9901885986328125, 0.9902305617806815
Epoch 19/20
----------
eval metrics, batch: 1024 acc, f1
0.810089111328125, 0.7854211923726768
eval metrics, batch: 2048 acc, f1
0.807708740234375, 0.7815262993654867
eval metrics, batch: 3072 acc, f1
0.819732666015625, 0.7978370238543413
eval metrics, batch: 4096 acc, f1
0.83245849609375, 0.8170853601652562
train metrics, batch: 4096  acc, f1 
0.9898033142089844, 0.9898543628517097
eval metrics, batch: 5120 acc, f1
0.789306640625, 0.7521361384361313
eval metrics, batch: 6144 acc, f1
0.8145751953125, 0.7905116535650255
eval metrics, batch: 7168 acc, f1
0.763458251953125, 0.7041038366100401
Epoch loss - train: tensor(0.0335, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(1.4860, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.766204833984375, 0.7122196761954848
train metrics acc, f1 
0.9827651977539062, 0.9825108774754967
Epoch 20/20
----------
eval metrics, batch: 1024 acc, f1
0.8104248046875, 0.7826604156462109
eval metrics, batch: 2048 acc, f1
0.802490234375, 0.772112676056338
eval metrics, batch: 3072 acc, f1
0.822021484375, 0.8143148242485991
eval metrics, batch: 4096 acc, f1
0.782806396484375, 0.7396378269617706
train metrics, batch: 4096  acc, f1 
0.9811973571777344, 0.9809522705403619
eval metrics, batch: 5120 acc, f1
0.76959228515625, 0.7191847057948375
eval metrics, batch: 6144 acc, f1
0.776397705078125, 0.7293813481071099
eval metrics, batch: 7168 acc, f1
0.769561767578125, 0.7168410394870064
Epoch loss - train: tensor(0.0308, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.9685, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.818817138671875, 0.7987662271633393
train metrics acc, f1 
0.9928855895996094, 0.9929017009275365
Training time 507m 6s
train_acc
0.5	0.8556747436523438	0.8839874267578125	0.8867301940917969	0.88201904296875	0.9135513305664062	0.9271888732910156	0.918212890625	0.8961715698242188	0.9456405639648438	0.94696044921875	0.9281883239746094	0.9497795104980469	0.9474754333496094	0.9515838623046875	0.9661788940429688	0.9604263305664062	0.9666366577148438	0.9735145568847656	0.9778938293457031	0.9763755798339844	0.9678497314453125	0.9799728393554688	0.9776687622070312	0.9789657592773438	0.984619140625	0.9850845336914062	0.9872283935546875	0.9851837158203125	0.9860877990722656	0.9774360656738281	0.9884109497070312	0.9873046875	0.9865188598632812	0.9937744140625	0.9880104064941406	0.9901885986328125	0.9898033142089844	0.9827651977539062	0.9811973571777344	0.9928855895996094
train_f1
0.0	0.8582401606666467	0.8861996707079779	0.880231043203627	0.8907330860272037	0.9130838325649896	0.9259582523556269	0.9133366209366436	0.8858391564395306	0.9450922065611923	0.9459682120234718	0.931493888126699	0.95062983060763	0.9455506036531595	0.9529221501802697	0.9658498255128689	0.9611466494883972	0.9660797393732548	0.9734849208137453	0.9777651586366723	0.9761804943903198	0.9684034520765695	0.9801428204003207	0.9775762079506017	0.9786873840445269	0.9846240676052901	0.9849970838321516	0.9873066424021838	0.9852647712700978	0.9859316836075376	0.9769477495313553	0.9884910292156625	0.9874102488443002	0.986564576711933	0.9937765032490314	0.9881102347311279	0.9902305617806815	0.9898543628517097	0.9825108774754967	0.9809522705403619	0.9929017009275365
train_loss
tensor(0.3670, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.2863, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.2371, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.2054, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.1821, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.1595, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.1384, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.1214, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.1062, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0916, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0797, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0695, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0357, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0335, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0308, device='cuda:0', grad_fn=<DivBackward0>)
valid_acc
0.500457763671875	0.75494384765625	0.804656982421875	0.8089599609375	0.807952880859375	0.7867431640625	0.804656982421875	0.754547119140625	0.8189697265625	0.7811279296875	0.81915283203125	0.82501220703125	0.772491455078125	0.82135009765625	0.771820068359375	0.8134765625	0.827117919921875	0.785797119140625	0.78741455078125	0.7666015625	0.795196533203125	0.7911376953125	0.795318603515625	0.83807373046875	0.807281494140625	0.81298828125	0.817169189453125	0.823486328125	0.75347900390625	0.76678466796875	0.7828369140625	0.78759765625	0.727752685546875	0.7835693359375	0.768310546875	0.833160400390625	0.802734375	0.78057861328125	0.7755126953125	0.80718994140625	0.79376220703125	0.804718017578125	0.811187744140625	0.840087890625	0.824981689453125	0.79742431640625	0.789825439453125	0.786346435546875	0.828460693359375	0.791412353515625	0.7923583984375	0.827178955078125	0.75592041015625	0.804046630859375	0.800933837890625	0.82928466796875	0.832061767578125	0.794281005859375	0.8070068359375	0.803497314453125	0.79541015625	0.820587158203125	0.775604248046875	0.7718505859375	0.819061279296875	0.806640625	0.783660888671875	0.80523681640625	0.79339599609375	0.796112060546875	0.79498291015625	0.81768798828125	0.8184814453125	0.79156494140625	0.804290771484375	0.803863525390625	0.79962158203125	0.778228759765625	0.772857666015625	0.790496826171875	0.797088623046875	0.79937744140625	0.80828857421875	0.789154052734375	0.823822021484375	0.809967041015625	0.8011474609375	0.798431396484375	0.814117431640625	0.805694580078125	0.7882080078125	0.803009033203125	0.796661376953125	0.79193115234375	0.776763916015625	0.80755615234375	0.78277587890625	0.80596923828125	0.805694580078125	0.773895263671875	0.812652587890625	0.769805908203125	0.795166015625	0.761199951171875	0.799102783203125	0.802734375	0.828369140625	0.795013427734375	0.819122314453125	0.795623779296875	0.805694580078125	0.804840087890625	0.818756103515625	0.818603515625	0.814849853515625	0.814178466796875	0.77618408203125	0.79425048828125	0.806549072265625	0.797271728515625	0.742889404296875	0.81591796875	0.8046875	0.816925048828125	0.8079833984375	0.802215576171875	0.7991943359375	0.793914794921875	0.825164794921875	0.815643310546875	0.807769775390625	0.811676025390625	0.7972412109375	0.789337158203125	0.808837890625	0.817230224609375	0.80633544921875	0.797210693359375	0.79925537109375	0.79791259765625	0.82427978515625	0.80389404296875	0.8033447265625	0.806884765625	0.823211669921875	0.810089111328125	0.807708740234375	0.819732666015625	0.83245849609375	0.789306640625	0.8145751953125	0.763458251953125	0.766204833984375	0.8104248046875	0.802490234375	0.822021484375	0.782806396484375	0.76959228515625	0.776397705078125	0.769561767578125	0.818817138671875
valid_f1
0.0	0.7055804062477085	0.8217041308041558	0.820228591120556	0.8014137397835212	0.7657862984314251	0.794305729618561	0.7053306466385785	0.8107090433339715	0.7664452259997395	0.8078594124894625	0.8132247557003257	0.7295483402865953	0.8168220789786594	0.7265679283232767	0.8044410315479619	0.8322227158300015	0.7535030728709394	0.747773191396915	0.7207128250073035	0.7694527465732248	0.757700205338809	0.7625420428394406	0.834011136832885	0.7798500958689211	0.7958694203864091	0.8124236826450422	0.8046077967704884	0.6884929816443005	0.7100910470409711	0.7404814004376368	0.745985401459854	0.6356544823361242	0.7454780361757106	0.7148223273983924	0.8213223518645619	0.7727784026996626	0.733999260081391	0.7267053053945609	0.7867701653729329	0.7578645646721606	0.7745163677367067	0.7843273956844564	0.8291712851274695	0.8196143805240147	0.7616345877621373	0.7493540051679587	0.7421077835488268	0.8157351253892804	0.7569432097009352	0.7591163350562912	0.8153630465260344	0.6939384662482779	0.7718762212669201	0.7717395107953949	0.8157808074820523	0.8233556960806343	0.758689815643458	0.7812067533905341	0.7711716834286932	0.7611685073031706	0.8015661389948358	0.7265627905247108	0.7256312389900176	0.8036819972848581	0.7830582756967747	0.7368694554767826	0.783675683004542	0.7552068267283772	0.7636633768438926	0.7563116656993616	0.8040283427371736	0.7949813870122708	0.7529837251356238	0.7802487749717301	0.7797992256826669	0.7664674918196045	0.7355435059499982	0.7262898540065458	0.7557721726137536	0.763641534250471	0.7703486341088521	0.7828101230811783	0.75276435856146	0.8095410906931477	0.7834764769289614	0.7775198033324229	0.7640650116092159	0.7949503450597543	0.7801526190393978	0.7550300035298271	0.7741822634248732	0.7655111736758754	0.7594892055876957	0.7301435053676172	0.7816028260718986	0.7407299482771181	0.7739779594738713	0.7774787683919897	0.721246096542383	0.7875263904752016	0.7157553604401402	0.7629273806159932	0.7008220225578283	0.7666182153366186	0.769554367201426	0.8151945320715037	0.7617324678088752	0.8013007475946227	0.7618675105785301	0.7834869248818308	0.775353918572382	0.7990798064887175	0.7983444157959018	0.7960466601674119	0.79140831078072	0.7276643148904567	0.7595749233292918	0.7785192690681667	0.7648412333179936	0.6683984728618098	0.79125138427464	0.7774222716839396	0.7987115391068014	0.78665400786654	0.7710298533827945	0.76415770609319	0.7581217092302733	0.8123423629991156	0.7927260250471779	0.7817016115058049	0.7872582480091013	0.7693055555555556	0.7498278548907331	0.7820155902004454	0.7956879200354792	0.7783908367090375	0.7642028316951137	0.7647689886997568	0.7648604502521128	0.8105173094642623	0.7774776646582173	0.7728266234224072	0.7792352777002511	0.8049297908879685	0.7854211923726768	0.7815262993654867	0.7978370238543413	0.8170853601652562	0.7521361384361313	0.7905116535650255	0.7041038366100401	0.7122196761954848	0.7826604156462109	0.772112676056338	0.8143148242485991	0.7396378269617706	0.7191847057948375	0.7293813481071099	0.7168410394870064	0.7987662271633393
valid_loss
tensor(0.4194, device='cuda:0')	tensor(0.3853, device='cuda:0')	tensor(0.4532, device='cuda:0')	tensor(0.7254, device='cuda:0')	tensor(0.5385, device='cuda:0')	tensor(0.4233, device='cuda:0')	tensor(0.4396, device='cuda:0')	tensor(0.4774, device='cuda:0')	tensor(0.5546, device='cuda:0')	tensor(0.6564, device='cuda:0')	tensor(0.6501, device='cuda:0')	tensor(0.8105, device='cuda:0')	tensor(0.8809, device='cuda:0')	tensor(0.7368, device='cuda:0')	tensor(1.2563, device='cuda:0')	tensor(0.7651, device='cuda:0')	tensor(0.9273, device='cuda:0')	tensor(0.9488, device='cuda:0')	tensor(1.4860, device='cuda:0')	tensor(0.9685, device='cuda:0')
Best model metrics: train, valid, test: acc, f1
0.88201904296875, 0.8907330860272037
0.827117919921875, 0.8322227158300015
0.785308837890625, 0.782742966554461
Model saved, path ./models/resnet_12-1559491145.pth
experiment validation
train set
Evaluation results
[[105155.  25917.]
 [  5011. 126061.]]
#############################
Accuracy
0.88201904296875
------------------------
Recall
0.9617691040039062
------------------------
Specificity
0.8022689819335938
------------------------
Precision
0.8294687388964193
------------------------
Fall_out
0.19773101806640625
------------------------
F1
0.8907330860272037
------------------------
#############################
valid set
Evaluation results
[[13053.  3346.]
 [ 2319. 14050.]]
#############################
Accuracy
0.827117919921875
------------------------
Recall
0.8583297696866027
------------------------
Specificity
0.7959631684858833
------------------------
Precision
0.8076569326281904
------------------------
Fall_out
0.20403683151411672
------------------------
F1
0.8322227158300015
------------------------
#############################
test set
Evaluation results
[[13060.  3331.]
 [ 3704. 12673.]]
#############################
Accuracy
0.785308837890625
------------------------
Recall
0.77382915063809
------------------------
Specificity
0.7967787200292844
------------------------
Precision
0.7918645338665333
------------------------
Fall_out
0.20322127997071562
------------------------
F1
0.782742966554461
------------------------
#############################
AUC: 0.8743620415916296
Experiment end
########################################
