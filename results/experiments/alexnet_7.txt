----------------------------------------
Starting experiment alexnet_7
Experiment parameters Experiment[name: alexnet_7, model: AlexNet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
    (1): ReLU(inplace)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU(inplace)
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace)
    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace)
    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace)
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Dropout(p=0.5)
    (1): Linear(in_features=9216, out_features=4096, bias=True)
    (2): ReLU(inplace)
    (3): Dropout(p=0.5)
    (4): Linear(in_features=4096, out_features=4096, bias=True)
    (5): ReLU(inplace)
    (6): Linear(in_features=4096, out_features=1, bias=True)
  )
), params: Params(lr: 1e-05, weight_decay: 0, batch_size: 32, num_epochs: 8), optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
), criterion: BCEWithLogitsLoss()]
start metrics
eval metrics acc, f1
0.5106201171875, 0.667854183927092
train metrics acc, f1
0.49529266357421875, 0.6543242777194275
Epoch 1/8
----------
eval metrics, batch: 1024 acc, f1
0.810089111328125, 0.8055980756614913
eval metrics, batch: 2048 acc, f1
0.820892333984375, 0.8108543620484063
eval metrics, batch: 3072 acc, f1
0.8282470703125, 0.816282561859372
eval metrics, batch: 4096 acc, f1
0.7596435546875, 0.7004411988437548
train metrics, batch: 4096  acc, f1 
0.8715629577636719, 0.8585900585900585
eval metrics, batch: 5120 acc, f1
0.822906494140625, 0.8028671399938853
eval metrics, batch: 6144 acc, f1
0.81756591796875, 0.7969567284831194
eval metrics, batch: 7168 acc, f1
0.830657958984375, 0.8208786597372414
Epoch loss - train: tensor(0.2840, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.3775, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.837738037109375, 0.8337970054077709
train metrics acc, f1 
0.9113693237304688, 0.9143781600554254
Epoch 2/8
----------
eval metrics, batch: 1024 acc, f1
0.830657958984375, 0.8169915240262524
eval metrics, batch: 2048 acc, f1
0.80535888671875, 0.7728308875908249
eval metrics, batch: 3072 acc, f1
0.832275390625, 0.8236539819033563
eval metrics, batch: 4096 acc, f1
0.83148193359375, 0.8239607243050242
train metrics, batch: 4096  acc, f1 
0.9214134216308594, 0.9234143639425562
eval metrics, batch: 5120 acc, f1
0.8338623046875, 0.820755959436323
eval metrics, batch: 6144 acc, f1
0.774658203125, 0.7243541884425863
eval metrics, batch: 7168 acc, f1
0.809722900390625, 0.7828055874873724
Epoch loss - train: tensor(0.1905, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4583, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.8096923828125, 0.7842960913178831
train metrics acc, f1 
0.9451065063476562, 0.9450398356160198
Epoch 3/8
----------
eval metrics, batch: 1024 acc, f1
0.821197509765625, 0.8001909763666746
eval metrics, batch: 2048 acc, f1
0.808197021484375, 0.7817177786267495
eval metrics, batch: 3072 acc, f1
0.844024658203125, 0.8376274740286559
eval metrics, batch: 4096 acc, f1
0.81658935546875, 0.7907090123972698
train metrics, batch: 4096  acc, f1 
0.9461021423339844, 0.9453150701897659
eval metrics, batch: 5120 acc, f1
0.79058837890625, 0.7513587941155156
eval metrics, batch: 6144 acc, f1
0.809417724609375, 0.7814523184601925
eval metrics, batch: 7168 acc, f1
0.819732666015625, 0.8043067748881895
Epoch loss - train: tensor(0.1500, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4763, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.8211669921875, 0.8033029001074113
train metrics acc, f1 
0.9549560546875, 0.9554307455385451
Epoch 4/8
----------
eval metrics, batch: 1024 acc, f1
0.800140380859375, 0.7687744942273065
eval metrics, batch: 2048 acc, f1
0.800262451171875, 0.7689972823209685
eval metrics, batch: 3072 acc, f1
0.808258056640625, 0.780980932129536
eval metrics, batch: 4096 acc, f1
0.8203125, 0.798134942402633
train metrics, batch: 4096  acc, f1 
0.9610557556152344, 0.9609722344341949
eval metrics, batch: 5120 acc, f1
0.788238525390625, 0.7453484531542441
eval metrics, batch: 6144 acc, f1
0.79449462890625, 0.757420749279539
eval metrics, batch: 7168 acc, f1
0.789459228515625, 0.7518256052375984
Epoch loss - train: tensor(0.1203, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.6186, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.7960205078125, 0.7626589020666146
train metrics acc, f1 
0.9681777954101562, 0.9681892021751234
Epoch 5/8
----------
eval metrics, batch: 1024 acc, f1
0.79498291015625, 0.7572450675724507
eval metrics, batch: 2048 acc, f1
0.80206298828125, 0.7708774904620602
eval metrics, batch: 3072 acc, f1
0.7874755859375, 0.743650150923949
eval metrics, batch: 4096 acc, f1
0.791778564453125, 0.7565388046387155
train metrics, batch: 4096  acc, f1 
0.9718246459960938, 0.9717024504620477
eval metrics, batch: 5120 acc, f1
0.788116455078125, 0.748578671012131
eval metrics, batch: 6144 acc, f1
0.807586669921875, 0.7780789130970399
eval metrics, batch: 7168 acc, f1
0.817108154296875, 0.7943306221901918
Epoch loss - train: tensor(0.0968, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.7445, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.777984619140625, 0.7272522775840737
train metrics acc, f1 
0.9697494506835938, 0.9690565566542061
Epoch 6/8
----------
eval metrics, batch: 1024 acc, f1
0.810791015625, 0.7875839386049062
eval metrics, batch: 2048 acc, f1
0.805755615234375, 0.7761561455952172
eval metrics, batch: 3072 acc, f1
0.80523681640625, 0.7785872883708022
eval metrics, batch: 4096 acc, f1
0.806793212890625, 0.7780076440267891
train metrics, batch: 4096  acc, f1 
0.980316162109375, 0.9802426024627826
eval metrics, batch: 5120 acc, f1
0.7960205078125, 0.7643325576475566
eval metrics, batch: 6144 acc, f1
0.796783447265625, 0.7633028827355774
eval metrics, batch: 7168 acc, f1
0.8011474609375, 0.773198746954403
Epoch loss - train: tensor(0.0766, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.7156, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.78955078125, 0.7516744688512783
train metrics acc, f1 
0.984527587890625, 0.9844383057090239
Epoch 7/8
----------
eval metrics, batch: 1024 acc, f1
0.81610107421875, 0.7975270479134466
eval metrics, batch: 2048 acc, f1
0.812530517578125, 0.7873879486380784
eval metrics, batch: 3072 acc, f1
0.80694580078125, 0.7763557943859153
eval metrics, batch: 4096 acc, f1
0.797149658203125, 0.763376170303656
train metrics, batch: 4096  acc, f1 
0.9854545593261719, 0.98543160726089
eval metrics, batch: 5120 acc, f1
0.776885986328125, 0.728124651370347
eval metrics, batch: 6144 acc, f1
0.805450439453125, 0.7778203743073223
eval metrics, batch: 7168 acc, f1
0.815521240234375, 0.7963892350702281
Epoch loss - train: tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.8523, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.7921142578125, 0.7576145744378024
train metrics acc, f1 
0.98370361328125, 0.9837402087282194
Epoch 8/8
----------
eval metrics, batch: 1024 acc, f1
0.7860107421875, 0.7481321839080459
eval metrics, batch: 2048 acc, f1
0.809356689453125, 0.7868136368289936
eval metrics, batch: 3072 acc, f1
0.813995361328125, 0.7900520133650236
eval metrics, batch: 4096 acc, f1
0.821380615234375, 0.8049715104461698
train metrics, batch: 4096  acc, f1 
0.9806327819824219, 0.9809150405419121
eval metrics, batch: 5120 acc, f1
0.795257568359375, 0.7589725166157715
eval metrics, batch: 6144 acc, f1
0.786468505859375, 0.74572082712505
eval metrics, batch: 7168 acc, f1
0.80206298828125, 0.7750572241104252
Epoch loss - train: tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.8916, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.80316162109375, 0.7774173510939333
train metrics acc, f1 
0.984954833984375, 0.9850244150636767
Training time 90m 20s
train_acc
0.49529266357421875	0.8715629577636719	0.9113693237304688	0.9214134216308594	0.9451065063476562	0.9461021423339844	0.9549560546875	0.9610557556152344	0.9681777954101562	0.9718246459960938	0.9697494506835938	0.980316162109375	0.984527587890625	0.9854545593261719	0.98370361328125	0.9806327819824219	0.984954833984375
train_f1
0.6543242777194275	0.8585900585900585	0.9143781600554254	0.9234143639425562	0.9450398356160198	0.9453150701897659	0.9554307455385451	0.9609722344341949	0.9681892021751234	0.9717024504620477	0.9690565566542061	0.9802426024627826	0.9844383057090239	0.98543160726089	0.9837402087282194	0.9809150405419121	0.9850244150636767
train_loss
tensor(0.2840, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.1905, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.1500, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.1203, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0968, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0766, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)
valid_acc
0.5106201171875	0.810089111328125	0.820892333984375	0.8282470703125	0.7596435546875	0.822906494140625	0.81756591796875	0.830657958984375	0.837738037109375	0.830657958984375	0.80535888671875	0.832275390625	0.83148193359375	0.8338623046875	0.774658203125	0.809722900390625	0.8096923828125	0.821197509765625	0.808197021484375	0.844024658203125	0.81658935546875	0.79058837890625	0.809417724609375	0.819732666015625	0.8211669921875	0.800140380859375	0.800262451171875	0.808258056640625	0.8203125	0.788238525390625	0.79449462890625	0.789459228515625	0.7960205078125	0.79498291015625	0.80206298828125	0.7874755859375	0.791778564453125	0.788116455078125	0.807586669921875	0.817108154296875	0.777984619140625	0.810791015625	0.805755615234375	0.80523681640625	0.806793212890625	0.7960205078125	0.796783447265625	0.8011474609375	0.78955078125	0.81610107421875	0.812530517578125	0.80694580078125	0.797149658203125	0.776885986328125	0.805450439453125	0.815521240234375	0.7921142578125	0.7860107421875	0.809356689453125	0.813995361328125	0.821380615234375	0.795257568359375	0.786468505859375	0.80206298828125	0.80316162109375
valid_f1
0.667854183927092	0.8055980756614913	0.8108543620484063	0.816282561859372	0.7004411988437548	0.8028671399938853	0.7969567284831194	0.8208786597372414	0.8337970054077709	0.8169915240262524	0.7728308875908249	0.8236539819033563	0.8239607243050242	0.820755959436323	0.7243541884425863	0.7828055874873724	0.7842960913178831	0.8001909763666746	0.7817177786267495	0.8376274740286559	0.7907090123972698	0.7513587941155156	0.7814523184601925	0.8043067748881895	0.8033029001074113	0.7687744942273065	0.7689972823209685	0.780980932129536	0.798134942402633	0.7453484531542441	0.757420749279539	0.7518256052375984	0.7626589020666146	0.7572450675724507	0.7708774904620602	0.743650150923949	0.7565388046387155	0.748578671012131	0.7780789130970399	0.7943306221901918	0.7272522775840737	0.7875839386049062	0.7761561455952172	0.7785872883708022	0.7780076440267891	0.7643325576475566	0.7633028827355774	0.773198746954403	0.7516744688512783	0.7975270479134466	0.7873879486380784	0.7763557943859153	0.763376170303656	0.728124651370347	0.7778203743073223	0.7963892350702281	0.7576145744378024	0.7481321839080459	0.7868136368289936	0.7900520133650236	0.8049715104461698	0.7589725166157715	0.74572082712505	0.7750572241104252	0.7774173510939333
valid_loss
tensor(0.3775, device='cuda:0')	tensor(0.4583, device='cuda:0')	tensor(0.4763, device='cuda:0')	tensor(0.6186, device='cuda:0')	tensor(0.7445, device='cuda:0')	tensor(0.7156, device='cuda:0')	tensor(0.8523, device='cuda:0')	tensor(0.8916, device='cuda:0')
Experiment end
########################################