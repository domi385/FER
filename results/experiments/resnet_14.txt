----------------------------------------
Starting experiment resnet_14-1559491145
Experiment parameters Experiment[name: resnet_14-1559491145, model: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1, bias=True)
), params: Params(lr: 1e-06, weight_decay: 0, batch_size: 32, num_epochs: 20), optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-06
    weight_decay: 0
), criterion: BCEWithLogitsLoss()]
start metrics
eval metrics acc, f1
0.499542236328125, 0.6662596414107496
train metrics acc, f1
0.5, 0.6666666666666666
Epoch 1/20
----------
eval metrics, batch: 1024 acc, f1
0.74853515625, 0.7449074360720699
eval metrics, batch: 2048 acc, f1
0.748443603515625, 0.7300121188300416
eval metrics, batch: 3072 acc, f1
0.763458251953125, 0.7568771368526709
eval metrics, batch: 4096 acc, f1
0.766265869140625, 0.7531186539019438
train metrics, batch: 4096  acc, f1 
0.8043365478515625, 0.8008093141024147
eval metrics, batch: 5120 acc, f1
0.761260986328125, 0.7402118686281672
eval metrics, batch: 6144 acc, f1
0.77783203125, 0.7796610169491526
eval metrics, batch: 7168 acc, f1
0.773468017578125, 0.7652063893721335
Epoch loss - train: tensor(0.4500, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4691, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.768829345703125, 0.7514519145585196
train metrics acc, f1 
0.8153228759765625, 0.8099462956565434
Epoch 2/20
----------
eval metrics, batch: 1024 acc, f1
0.76544189453125, 0.753716995642143
eval metrics, batch: 2048 acc, f1
0.771728515625, 0.7606859482979268
eval metrics, batch: 3072 acc, f1
0.76953125, 0.7606187396982376
eval metrics, batch: 4096 acc, f1
0.776641845703125, 0.7683494223769584
train metrics, batch: 4096  acc, f1 
0.8211288452148438, 0.8219101080920947
eval metrics, batch: 5120 acc, f1
0.776763916015625, 0.7710700090758301
eval metrics, batch: 6144 acc, f1
0.777984619140625, 0.7663101088946709
eval metrics, batch: 7168 acc, f1
0.76824951171875, 0.7471027041428
Epoch loss - train: tensor(0.4130, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4616, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.7783203125, 0.7682638933197218
train metrics acc, f1 
0.8276329040527344, 0.8273252903388528
Epoch 3/20
----------
eval metrics, batch: 1024 acc, f1
0.7586669921875, 0.7434799532892176
eval metrics, batch: 2048 acc, f1
0.77691650390625, 0.7715910511186101
eval metrics, batch: 3072 acc, f1
0.773193359375, 0.7576310983563788
eval metrics, batch: 4096 acc, f1
0.7757568359375, 0.7617689015691869
train metrics, batch: 4096  acc, f1 
0.8296890258789062, 0.8274297288104147
eval metrics, batch: 5120 acc, f1
0.782318115234375, 0.7753598085220295
eval metrics, batch: 6144 acc, f1
0.773681640625, 0.7541766109785203
eval metrics, batch: 7168 acc, f1
0.777923583984375, 0.7644450199074224
Epoch loss - train: tensor(0.3947, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4818, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.769378662109375, 0.7453240319482358
train metrics acc, f1 
0.8337745666503906, 0.827204701459689
Epoch 4/20
----------
eval metrics, batch: 1024 acc, f1
0.78399658203125, 0.7790472622838235
eval metrics, batch: 2048 acc, f1
0.779388427734375, 0.7701503926743187
eval metrics, batch: 3072 acc, f1
0.779693603515625, 0.7717023497043104
eval metrics, batch: 4096 acc, f1
0.766815185546875, 0.7410795974382434
train metrics, batch: 4096  acc, f1 
0.8365020751953125, 0.8301995927325743
eval metrics, batch: 5120 acc, f1
0.779937744140625, 0.7657473280706883
eval metrics, batch: 6144 acc, f1
0.783294677734375, 0.7753914281195635
eval metrics, batch: 7168 acc, f1
0.78741455078125, 0.7822036018009004
Epoch loss - train: tensor(0.3779, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4667, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.779510498046875, 0.7636803715696857
train metrics acc, f1 
0.8462066650390625, 0.844968275331667
Epoch 5/20
----------
eval metrics, batch: 1024 acc, f1
0.78570556640625, 0.7789321244175796
eval metrics, batch: 2048 acc, f1
0.778717041015625, 0.7629849965678424
eval metrics, batch: 3072 acc, f1
0.7877197265625, 0.7836930157348094
eval metrics, batch: 4096 acc, f1
0.783721923828125, 0.7818512020192693
train metrics, batch: 4096  acc, f1 
0.847015380859375, 0.8531354827369007
eval metrics, batch: 5120 acc, f1
0.773345947265625, 0.7505457965270547
eval metrics, batch: 6144 acc, f1
0.77532958984375, 0.7569494882799603
eval metrics, batch: 7168 acc, f1
0.7867431640625, 0.7884859858344936
Epoch loss - train: tensor(0.3631, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4604, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.77899169921875, 0.7613838550247117
train metrics acc, f1 
0.8549728393554688, 0.8540923081646594
Epoch 6/20
----------
eval metrics, batch: 1024 acc, f1
0.7713623046875, 0.7515585621435203
eval metrics, batch: 2048 acc, f1
0.78045654296875, 0.7654385392892077
eval metrics, batch: 3072 acc, f1
0.776611328125, 0.7577763070814031
eval metrics, batch: 4096 acc, f1
0.779510498046875, 0.7628426062694896
train metrics, batch: 4096  acc, f1 
0.8584098815917969, 0.8576905823578803
eval metrics, batch: 5120 acc, f1
0.78045654296875, 0.7689342840624398
eval metrics, batch: 6144 acc, f1
0.78240966796875, 0.7678583056586573
eval metrics, batch: 7168 acc, f1
0.788604736328125, 0.7843332606868209
Epoch loss - train: tensor(0.3480, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4551, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.7822265625, 0.7698806836504354
train metrics acc, f1 
0.8637275695800781, 0.8654698556520888
Epoch 7/20
----------
eval metrics, batch: 1024 acc, f1
0.774932861328125, 0.7552679608428737
eval metrics, batch: 2048 acc, f1
0.779205322265625, 0.7617008662428774
eval metrics, batch: 3072 acc, f1
0.76495361328125, 0.7329588794119687
eval metrics, batch: 4096 acc, f1
0.765289306640625, 0.7368709158712238
train metrics, batch: 4096  acc, f1 
0.8619499206542969, 0.8582319043832193
eval metrics, batch: 5120 acc, f1
0.777069091796875, 0.7566379051870606
eval metrics, batch: 6144 acc, f1
0.7802734375, 0.7689196995956095
eval metrics, batch: 7168 acc, f1
0.786468505859375, 0.7878732757313931
Epoch loss - train: tensor(0.3353, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4705, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.780059814453125, 0.7695308752518307
train metrics acc, f1 
0.8696212768554688, 0.8726317358574942
Epoch 8/20
----------
eval metrics, batch: 1024 acc, f1
0.78228759765625, 0.768796992481203
eval metrics, batch: 2048 acc, f1
0.781646728515625, 0.7664588569376897
eval metrics, batch: 3072 acc, f1
0.768951416015625, 0.7411535437108961
eval metrics, batch: 4096 acc, f1
0.775238037109375, 0.7577382322949903
train metrics, batch: 4096  acc, f1 
0.8731269836425781, 0.8737650349756518
eval metrics, batch: 5120 acc, f1
0.770599365234375, 0.7464156799244341
eval metrics, batch: 6144 acc, f1
0.779571533203125, 0.7666160457526899
eval metrics, batch: 7168 acc, f1
0.776031494140625, 0.758609347761734
Epoch loss - train: tensor(0.3228, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4818, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.774810791015625, 0.7581844994265116
train metrics acc, f1 
0.8770637512207031, 0.8783018832299262
Epoch 9/20
----------
eval metrics, batch: 1024 acc, f1
0.779510498046875, 0.7629048666032225
eval metrics, batch: 2048 acc, f1
0.775146484375, 0.7554596747427813
eval metrics, batch: 3072 acc, f1
0.7822265625, 0.7690913797566657
eval metrics, batch: 4096 acc, f1
0.776275634765625, 0.7572114588508031
train metrics, batch: 4096  acc, f1 
0.8777618408203125, 0.8781170599601381
eval metrics, batch: 5120 acc, f1
0.77593994140625, 0.7566134058211231
eval metrics, batch: 6144 acc, f1
0.77587890625, 0.7543976991505585
eval metrics, batch: 7168 acc, f1
0.783721923828125, 0.7871452169995495
Epoch loss - train: tensor(0.3117, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4591, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.786773681640625, 0.7815948235441218
train metrics acc, f1 
0.8778724670410156, 0.8824900438620639
Epoch 10/20
----------
eval metrics, batch: 1024 acc, f1
0.780517578125, 0.7667661175249708
eval metrics, batch: 2048 acc, f1
0.78265380859375, 0.7703469624661421
eval metrics, batch: 3072 acc, f1
0.77740478515625, 0.7632279426085827
eval metrics, batch: 4096 acc, f1
0.76971435546875, 0.749385586183992
train metrics, batch: 4096  acc, f1 
0.8832168579101562, 0.8838909832894647
eval metrics, batch: 5120 acc, f1
0.77685546875, 0.7564290473017988
eval metrics, batch: 6144 acc, f1
0.766845703125, 0.7373487348734874
eval metrics, batch: 7168 acc, f1
0.773162841796875, 0.7525385358058395
Epoch loss - train: tensor(0.3010, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.5220, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.768646240234375, 0.7512876874118303
train metrics acc, f1 
0.8832206726074219, 0.8852977432734712
Epoch 11/20
----------
eval metrics, batch: 1024 acc, f1
0.778778076171875, 0.7584391349261889
eval metrics, batch: 2048 acc, f1
0.766082763671875, 0.7376347766558274
eval metrics, batch: 3072 acc, f1
0.7816162109375, 0.7683092663342614
eval metrics, batch: 4096 acc, f1
0.774200439453125, 0.7541615443399674
train metrics, batch: 4096  acc, f1 
0.8888130187988281, 0.8893343812528713
eval metrics, batch: 5120 acc, f1
0.780517578125, 0.7702530028111424
eval metrics, batch: 6144 acc, f1
0.784149169921875, 0.7844451894066377
eval metrics, batch: 7168 acc, f1
0.775604248046875, 0.7566923662353993
Epoch loss - train: tensor(0.2900, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4926, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.77825927734375, 0.765279751905931
train metrics acc, f1 
0.8917808532714844, 0.8943682162604956
Epoch 12/20
----------
eval metrics, batch: 1024 acc, f1
0.773529052734375, 0.7561688845079678
eval metrics, batch: 2048 acc, f1
0.779327392578125, 0.7642552081635314
eval metrics, batch: 3072 acc, f1
0.781402587890625, 0.7735162993644671
eval metrics, batch: 4096 acc, f1
0.783660888671875, 0.7831513260530422
train metrics, batch: 4096  acc, f1 
0.8831520080566406, 0.8896899679848459
eval metrics, batch: 5120 acc, f1
0.77142333984375, 0.7479472338134339
eval metrics, batch: 6144 acc, f1
0.76275634765625, 0.7364924411904278
eval metrics, batch: 7168 acc, f1
0.778564453125, 0.7668230606080082
Epoch loss - train: tensor(0.2805, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4984, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.78277587890625, 0.7684751496226906
train metrics acc, f1 
0.8961753845214844, 0.8978965572866452
Epoch 13/20
----------
eval metrics, batch: 1024 acc, f1
0.780364990234375, 0.7677712884385789
eval metrics, batch: 2048 acc, f1
0.7674560546875, 0.7362591720891596
eval metrics, batch: 3072 acc, f1
0.76910400390625, 0.7466514867398875
eval metrics, batch: 4096 acc, f1
0.776123046875, 0.760574412532637
train metrics, batch: 4096  acc, f1 
0.8979759216308594, 0.9001228625097561
eval metrics, batch: 5120 acc, f1
0.7738037109375, 0.7533608412085718
eval metrics, batch: 6144 acc, f1
0.780548095703125, 0.7800177429716418
eval metrics, batch: 7168 acc, f1
0.771270751953125, 0.7481941878044683
Epoch loss - train: tensor(0.2699, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.5477, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.76788330078125, 0.7391453460456822
train metrics acc, f1 
0.9028244018554688, 0.9014507331037951
Epoch 14/20
----------
eval metrics, batch: 1024 acc, f1
0.77880859375, 0.7597135658400742
eval metrics, batch: 2048 acc, f1
0.773406982421875, 0.7569637655068574
eval metrics, batch: 3072 acc, f1
0.76885986328125, 0.7431497558328811
eval metrics, batch: 4096 acc, f1
0.768310546875, 0.744738080828458
train metrics, batch: 4096  acc, f1 
0.9050636291503906, 0.9052533797288603
eval metrics, batch: 5120 acc, f1
0.77764892578125, 0.7577148177706837
eval metrics, batch: 6144 acc, f1
0.77813720703125, 0.7657558963783992
eval metrics, batch: 7168 acc, f1
0.7603759765625, 0.7264302139223747
Epoch loss - train: tensor(0.2600, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.5685, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.769256591796875, 0.7424113378530304
train metrics acc, f1 
0.9073753356933594, 0.9066700491614872
Epoch 15/20
----------
eval metrics, batch: 1024 acc, f1
0.77825927734375, 0.7663214768122467
eval metrics, batch: 2048 acc, f1
0.775665283203125, 0.7563877381938691
eval metrics, batch: 3072 acc, f1
0.785369873046875, 0.7793845478214498
eval metrics, batch: 4096 acc, f1
0.75, 0.7057893980749892
train metrics, batch: 4096  acc, f1 
0.9017715454101562, 0.8978450259453798
eval metrics, batch: 5120 acc, f1
0.77801513671875, 0.7709986147840322
eval metrics, batch: 6144 acc, f1
0.758087158203125, 0.7251291653663442
eval metrics, batch: 7168 acc, f1
0.77392578125, 0.753641503159295
Epoch loss - train: tensor(0.2512, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.6207, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.75408935546875, 0.7136257018977895
train metrics acc, f1 
0.9070549011230469, 0.9037971784717315
Epoch 16/20
----------
eval metrics, batch: 1024 acc, f1
0.764251708984375, 0.7381622207911059
eval metrics, batch: 2048 acc, f1
0.753173828125, 0.7141443415565137
eval metrics, batch: 3072 acc, f1
0.7659912109375, 0.7382220401474805
eval metrics, batch: 4096 acc, f1
0.772064208984375, 0.7565753022846528
train metrics, batch: 4096  acc, f1 
0.9120368957519531, 0.9139367971604629
eval metrics, batch: 5120 acc, f1
0.771453857421875, 0.7499248672655023
eval metrics, batch: 6144 acc, f1
0.778289794921875, 0.7670055482505372
eval metrics, batch: 7168 acc, f1
0.76654052734375, 0.7415191242059738
Epoch loss - train: tensor(0.2395, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.5503, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.77557373046875, 0.7602842427798422
train metrics acc, f1 
0.9185104370117188, 0.9200595754872317
Epoch 17/20
----------
eval metrics, batch: 1024 acc, f1
0.778472900390625, 0.7628630230962726
eval metrics, batch: 2048 acc, f1
0.77239990234375, 0.7603624445729709
eval metrics, batch: 3072 acc, f1
0.772186279296875, 0.7491009309985548
eval metrics, batch: 4096 acc, f1
0.780853271484375, 0.768377253814147
train metrics, batch: 4096  acc, f1 
0.9166450500488281, 0.9184046124489819
eval metrics, batch: 5120 acc, f1
0.769134521484375, 0.7443306634222178
eval metrics, batch: 6144 acc, f1
0.7725830078125, 0.7500167728950017
eval metrics, batch: 7168 acc, f1
0.755859375, 0.7172745264348318
Epoch loss - train: tensor(0.2303, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.6265, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.76300048828125, 0.7382717713669452
train metrics acc, f1 
0.9213523864746094, 0.9218819267886982
Epoch 18/20
----------
eval metrics, batch: 1024 acc, f1
0.7650146484375, 0.7373985403451333
eval metrics, batch: 2048 acc, f1
0.76495361328125, 0.7337711717939854
eval metrics, batch: 3072 acc, f1
0.77313232421875, 0.7564379791625713
eval metrics, batch: 4096 acc, f1
0.770660400390625, 0.7462777271346095
train metrics, batch: 4096  acc, f1 
0.9225921630859375, 0.9223884707178264
eval metrics, batch: 5120 acc, f1
0.7567138671875, 0.7239803337718994
eval metrics, batch: 6144 acc, f1
0.759613037109375, 0.7302859099469269
eval metrics, batch: 7168 acc, f1
0.777557373046875, 0.7628282302411089
Epoch loss - train: tensor(0.2218, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.6316, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.771148681640625, 0.748634062950424
train metrics acc, f1 
0.9249916076660156, 0.9254112943300749
Epoch 19/20
----------
eval metrics, batch: 1024 acc, f1
0.7686767578125, 0.7446951835634894
eval metrics, batch: 2048 acc, f1
0.758148193359375, 0.7267335609116927
eval metrics, batch: 3072 acc, f1
0.773101806640625, 0.753620306856215
eval metrics, batch: 4096 acc, f1
0.777557373046875, 0.7635207474937546
train metrics, batch: 4096  acc, f1 
0.9259719848632812, 0.9275738779288055
eval metrics, batch: 5120 acc, f1
0.77593994140625, 0.7693081128636964
eval metrics, batch: 6144 acc, f1
0.775726318359375, 0.7637357338048545
eval metrics, batch: 7168 acc, f1
0.7664794921875, 0.7406804934255117
Epoch loss - train: tensor(0.2122, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.6151, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.76922607421875, 0.7472762515874607
train metrics acc, f1 
0.9328651428222656, 0.9331951609290955
Epoch 20/20
----------
eval metrics, batch: 1024 acc, f1
0.769744873046875, 0.7482734461014914
eval metrics, batch: 2048 acc, f1
0.763336181640625, 0.7311306036126617
eval metrics, batch: 3072 acc, f1
0.780670166015625, 0.7701851437342117
eval metrics, batch: 4096 acc, f1
0.77789306640625, 0.7642065703362924
train metrics, batch: 4096  acc, f1 
0.9313926696777344, 0.9328865321536389
eval metrics, batch: 5120 acc, f1
0.7552490234375, 0.7186359809149593
eval metrics, batch: 6144 acc, f1
0.742584228515625, 0.6941069809610154
eval metrics, batch: 7168 acc, f1
0.745391845703125, 0.6999892121255709
Epoch loss - train: tensor(0.2013, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.6078, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.775054931640625, 0.7594085582792048
train metrics acc, f1 
0.9363632202148438, 0.9375687671684019
Training time 506m 16s
train_acc
0.5	0.8043365478515625	0.8153228759765625	0.8211288452148438	0.8276329040527344	0.8296890258789062	0.8337745666503906	0.8365020751953125	0.8462066650390625	0.847015380859375	0.8549728393554688	0.8584098815917969	0.8637275695800781	0.8619499206542969	0.8696212768554688	0.8731269836425781	0.8770637512207031	0.8777618408203125	0.8778724670410156	0.8832168579101562	0.8832206726074219	0.8888130187988281	0.8917808532714844	0.8831520080566406	0.8961753845214844	0.8979759216308594	0.9028244018554688	0.9050636291503906	0.9073753356933594	0.9017715454101562	0.9070549011230469	0.9120368957519531	0.9185104370117188	0.9166450500488281	0.9213523864746094	0.9225921630859375	0.9249916076660156	0.9259719848632812	0.9328651428222656	0.9313926696777344	0.9363632202148438
train_f1
0.6666666666666666	0.8008093141024147	0.8099462956565434	0.8219101080920947	0.8273252903388528	0.8274297288104147	0.827204701459689	0.8301995927325743	0.844968275331667	0.8531354827369007	0.8540923081646594	0.8576905823578803	0.8654698556520888	0.8582319043832193	0.8726317358574942	0.8737650349756518	0.8783018832299262	0.8781170599601381	0.8824900438620639	0.8838909832894647	0.8852977432734712	0.8893343812528713	0.8943682162604956	0.8896899679848459	0.8978965572866452	0.9001228625097561	0.9014507331037951	0.9052533797288603	0.9066700491614872	0.8978450259453798	0.9037971784717315	0.9139367971604629	0.9200595754872317	0.9184046124489819	0.9218819267886982	0.9223884707178264	0.9254112943300749	0.9275738779288055	0.9331951609290955	0.9328865321536389	0.9375687671684019
train_loss
tensor(0.4500, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.4130, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3947, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3779, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3631, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3480, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3353, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3228, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3117, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3010, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.2900, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.2805, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.2699, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.2600, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.2512, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.2395, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.2303, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.2218, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.2122, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.2013, device='cuda:0', grad_fn=<DivBackward0>)
valid_acc
0.499542236328125	0.74853515625	0.748443603515625	0.763458251953125	0.766265869140625	0.761260986328125	0.77783203125	0.773468017578125	0.768829345703125	0.76544189453125	0.771728515625	0.76953125	0.776641845703125	0.776763916015625	0.777984619140625	0.76824951171875	0.7783203125	0.7586669921875	0.77691650390625	0.773193359375	0.7757568359375	0.782318115234375	0.773681640625	0.777923583984375	0.769378662109375	0.78399658203125	0.779388427734375	0.779693603515625	0.766815185546875	0.779937744140625	0.783294677734375	0.78741455078125	0.779510498046875	0.78570556640625	0.778717041015625	0.7877197265625	0.783721923828125	0.773345947265625	0.77532958984375	0.7867431640625	0.77899169921875	0.7713623046875	0.78045654296875	0.776611328125	0.779510498046875	0.78045654296875	0.78240966796875	0.788604736328125	0.7822265625	0.774932861328125	0.779205322265625	0.76495361328125	0.765289306640625	0.777069091796875	0.7802734375	0.786468505859375	0.780059814453125	0.78228759765625	0.781646728515625	0.768951416015625	0.775238037109375	0.770599365234375	0.779571533203125	0.776031494140625	0.774810791015625	0.779510498046875	0.775146484375	0.7822265625	0.776275634765625	0.77593994140625	0.77587890625	0.783721923828125	0.786773681640625	0.780517578125	0.78265380859375	0.77740478515625	0.76971435546875	0.77685546875	0.766845703125	0.773162841796875	0.768646240234375	0.778778076171875	0.766082763671875	0.7816162109375	0.774200439453125	0.780517578125	0.784149169921875	0.775604248046875	0.77825927734375	0.773529052734375	0.779327392578125	0.781402587890625	0.783660888671875	0.77142333984375	0.76275634765625	0.778564453125	0.78277587890625	0.780364990234375	0.7674560546875	0.76910400390625	0.776123046875	0.7738037109375	0.780548095703125	0.771270751953125	0.76788330078125	0.77880859375	0.773406982421875	0.76885986328125	0.768310546875	0.77764892578125	0.77813720703125	0.7603759765625	0.769256591796875	0.77825927734375	0.775665283203125	0.785369873046875	0.75	0.77801513671875	0.758087158203125	0.77392578125	0.75408935546875	0.764251708984375	0.753173828125	0.7659912109375	0.772064208984375	0.771453857421875	0.778289794921875	0.76654052734375	0.77557373046875	0.778472900390625	0.77239990234375	0.772186279296875	0.780853271484375	0.769134521484375	0.7725830078125	0.755859375	0.76300048828125	0.7650146484375	0.76495361328125	0.77313232421875	0.770660400390625	0.7567138671875	0.759613037109375	0.777557373046875	0.771148681640625	0.7686767578125	0.758148193359375	0.773101806640625	0.777557373046875	0.77593994140625	0.775726318359375	0.7664794921875	0.76922607421875	0.769744873046875	0.763336181640625	0.780670166015625	0.77789306640625	0.7552490234375	0.742584228515625	0.745391845703125	0.775054931640625
valid_f1
0.6662596414107496	0.7449074360720699	0.7300121188300416	0.7568771368526709	0.7531186539019438	0.7402118686281672	0.7796610169491526	0.7652063893721335	0.7514519145585196	0.753716995642143	0.7606859482979268	0.7606187396982376	0.7683494223769584	0.7710700090758301	0.7663101088946709	0.7471027041428	0.7682638933197218	0.7434799532892176	0.7715910511186101	0.7576310983563788	0.7617689015691869	0.7753598085220295	0.7541766109785203	0.7644450199074224	0.7453240319482358	0.7790472622838235	0.7701503926743187	0.7717023497043104	0.7410795974382434	0.7657473280706883	0.7753914281195635	0.7822036018009004	0.7636803715696857	0.7789321244175796	0.7629849965678424	0.7836930157348094	0.7818512020192693	0.7505457965270547	0.7569494882799603	0.7884859858344936	0.7613838550247117	0.7515585621435203	0.7654385392892077	0.7577763070814031	0.7628426062694896	0.7689342840624398	0.7678583056586573	0.7843332606868209	0.7698806836504354	0.7552679608428737	0.7617008662428774	0.7329588794119687	0.7368709158712238	0.7566379051870606	0.7689196995956095	0.7878732757313931	0.7695308752518307	0.768796992481203	0.7664588569376897	0.7411535437108961	0.7577382322949903	0.7464156799244341	0.7666160457526899	0.758609347761734	0.7581844994265116	0.7629048666032225	0.7554596747427813	0.7690913797566657	0.7572114588508031	0.7566134058211231	0.7543976991505585	0.7871452169995495	0.7815948235441218	0.7667661175249708	0.7703469624661421	0.7632279426085827	0.749385586183992	0.7564290473017988	0.7373487348734874	0.7525385358058395	0.7512876874118303	0.7584391349261889	0.7376347766558274	0.7683092663342614	0.7541615443399674	0.7702530028111424	0.7844451894066377	0.7566923662353993	0.765279751905931	0.7561688845079678	0.7642552081635314	0.7735162993644671	0.7831513260530422	0.7479472338134339	0.7364924411904278	0.7668230606080082	0.7684751496226906	0.7677712884385789	0.7362591720891596	0.7466514867398875	0.760574412532637	0.7533608412085718	0.7800177429716418	0.7481941878044683	0.7391453460456822	0.7597135658400742	0.7569637655068574	0.7431497558328811	0.744738080828458	0.7577148177706837	0.7657558963783992	0.7264302139223747	0.7424113378530304	0.7663214768122467	0.7563877381938691	0.7793845478214498	0.7057893980749892	0.7709986147840322	0.7251291653663442	0.753641503159295	0.7136257018977895	0.7381622207911059	0.7141443415565137	0.7382220401474805	0.7565753022846528	0.7499248672655023	0.7670055482505372	0.7415191242059738	0.7602842427798422	0.7628630230962726	0.7603624445729709	0.7491009309985548	0.768377253814147	0.7443306634222178	0.7500167728950017	0.7172745264348318	0.7382717713669452	0.7373985403451333	0.7337711717939854	0.7564379791625713	0.7462777271346095	0.7239803337718994	0.7302859099469269	0.7628282302411089	0.748634062950424	0.7446951835634894	0.7267335609116927	0.753620306856215	0.7635207474937546	0.7693081128636964	0.7637357338048545	0.7406804934255117	0.7472762515874607	0.7482734461014914	0.7311306036126617	0.7701851437342117	0.7642065703362924	0.7186359809149593	0.6941069809610154	0.6999892121255709	0.7594085582792048
valid_loss
tensor(0.4691, device='cuda:0')	tensor(0.4616, device='cuda:0')	tensor(0.4818, device='cuda:0')	tensor(0.4667, device='cuda:0')	tensor(0.4604, device='cuda:0')	tensor(0.4551, device='cuda:0')	tensor(0.4705, device='cuda:0')	tensor(0.4818, device='cuda:0')	tensor(0.4591, device='cuda:0')	tensor(0.5220, device='cuda:0')	tensor(0.4926, device='cuda:0')	tensor(0.4984, device='cuda:0')	tensor(0.5477, device='cuda:0')	tensor(0.5685, device='cuda:0')	tensor(0.6207, device='cuda:0')	tensor(0.5503, device='cuda:0')	tensor(0.6265, device='cuda:0')	tensor(0.6316, device='cuda:0')	tensor(0.6151, device='cuda:0')	tensor(0.6078, device='cuda:0')
Best model metrics: train, valid, test: acc, f1
0.8778724670410156, 0.8824900438620639
0.786773681640625, 0.7815948235441218
0.76129150390625, 0.7492144918243027
Model saved, path ./models/resnet_14-1559491145.pth
experiment validation
train set
Evaluation results
[[109914.  21158.]
 [ 10857. 120215.]]
#############################
Accuracy
0.8778724670410156
------------------------
Recall
0.9171676635742188
------------------------
Specificity
0.8385772705078125
------------------------
Precision
0.8503391736753129
------------------------
Fall_out
0.1614227294921875
------------------------
F1
0.8824900438620639
------------------------
#############################
valid set
Evaluation results
[[13279.  3120.]
 [ 3867. 12502.]]
#############################
Accuracy
0.786773681640625
------------------------
Recall
0.7637607673040503
------------------------
Specificity
0.8097444966156473
------------------------
Precision
0.8002816540775829
------------------------
Fall_out
0.1902555033843527
------------------------
F1
0.7815948235441218
------------------------
#############################
test set
Evaluation results
[[13262.  3129.]
 [ 4693. 11684.]]
#############################
Accuracy
0.76129150390625
------------------------
Recall
0.7134395798986384
------------------------
Specificity
0.8091025562808859
------------------------
Precision
0.7887666239114292
------------------------
Fall_out
0.19089744371911416
------------------------
F1
0.7492144918243027
------------------------
#############################
AUC: 0.8381587623424059
Experiment end
########################################
