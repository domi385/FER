----------------------------------------
Starting experiment alexnet_10
Experiment parameters Experiment[name: alexnet_10, model: AlexNet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
    (1): ReLU(inplace)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU(inplace)
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace)
    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace)
    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace)
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Dropout(p=0.5)
    (1): Linear(in_features=9216, out_features=4096, bias=True)
    (2): ReLU(inplace)
    (3): Dropout(p=0.5)
    (4): Linear(in_features=4096, out_features=4096, bias=True)
    (5): ReLU(inplace)
    (6): Linear(in_features=4096, out_features=1, bias=True)
  )
), params: Params(lr: 1e-06, weight_decay: 0, batch_size: 32, num_epochs: 8), optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-06
    weight_decay: 0
), criterion: BCEWithLogitsLoss()]
start metrics
eval metrics acc, f1
0.66485595703125, 0.6916381198405122
train metrics acc, f1
0.6578750610351562, 0.689145067483727
Epoch 1/8
----------
eval metrics, batch: 1024 acc, f1
0.771759033203125, 0.7784722016527946
eval metrics, batch: 2048 acc, f1
0.777191162109375, 0.7744028674721132
eval metrics, batch: 3072 acc, f1
0.781219482421875, 0.777367162510481
eval metrics, batch: 4096 acc, f1
0.781341552734375, 0.7730943408176838
train metrics, batch: 4096  acc, f1 
0.8055343627929688, 0.805558098376663
eval metrics, batch: 5120 acc, f1
0.7808837890625, 0.7733871985860371
eval metrics, batch: 6144 acc, f1
0.781585693359375, 0.7754525774166222
eval metrics, batch: 7168 acc, f1
0.783203125, 0.7729771187523968
Epoch loss - train: tensor(0.4409, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4451, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.7818603515625, 0.7717169136433316
train metrics acc, f1 
0.8211212158203125, 0.8209901050574151
Epoch 2/8
----------
eval metrics, batch: 1024 acc, f1
0.78369140625, 0.786570310147546
eval metrics, batch: 2048 acc, f1
0.782958984375, 0.7735752944922
eval metrics, batch: 3072 acc, f1
0.77764892578125, 0.7581972653657242
eval metrics, batch: 4096 acc, f1
0.7799072265625, 0.7667227325656618
train metrics, batch: 4096  acc, f1 
0.8280792236328125, 0.8270168193018953
eval metrics, batch: 5120 acc, f1
0.783294677734375, 0.7709871964395136
eval metrics, batch: 6144 acc, f1
0.783355712890625, 0.7729700342191947
eval metrics, batch: 7168 acc, f1
0.783172607421875, 0.7767057418523524
Epoch loss - train: tensor(0.3870, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4436, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.78265380859375, 0.7727939769029541
train metrics acc, f1 
0.8340263366699219, 0.8351800711420226
Epoch 3/8
----------
eval metrics, batch: 1024 acc, f1
0.78515625, 0.7922323220399008
eval metrics, batch: 2048 acc, f1
0.78070068359375, 0.7709422414892261
eval metrics, batch: 3072 acc, f1
0.783721923828125, 0.7688443850092957
eval metrics, batch: 4096 acc, f1
0.781005859375, 0.7668767461503476
train metrics, batch: 4096  acc, f1 
0.8379669189453125, 0.8368328454759875
eval metrics, batch: 5120 acc, f1
0.7867431640625, 0.7773955147808359
eval metrics, batch: 6144 acc, f1
0.78582763671875, 0.7749486916367367
eval metrics, batch: 7168 acc, f1
0.787933349609375, 0.7831486971446403
Epoch loss - train: tensor(0.3688, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4324, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.787506103515625, 0.7757704569606801
train metrics acc, f1 
0.8420448303222656, 0.8416188862411499
Epoch 4/8
----------
eval metrics, batch: 1024 acc, f1
0.787384033203125, 0.7948166691208953
eval metrics, batch: 2048 acc, f1
0.783905029296875, 0.7692357829558416
eval metrics, batch: 3072 acc, f1
0.784332275390625, 0.7673109216028449
eval metrics, batch: 4096 acc, f1
0.7867431640625, 0.7765413149143003
train metrics, batch: 4096  acc, f1 
0.8455238342285156, 0.8467207437043655
eval metrics, batch: 5120 acc, f1
0.78863525390625, 0.7736157416486893
eval metrics, batch: 6144 acc, f1
0.786834716796875, 0.7727494550541693
eval metrics, batch: 7168 acc, f1
0.7891845703125, 0.7709549071618037
Epoch loss - train: tensor(0.3566, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4348, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.787841796875, 0.7759009735026755
train metrics acc, f1 
0.8484382629394531, 0.8486409264938379
Epoch 5/8
----------
eval metrics, batch: 1024 acc, f1
0.791900634765625, 0.7937573722892659
eval metrics, batch: 2048 acc, f1
0.7908935546875, 0.7876534027519524
eval metrics, batch: 3072 acc, f1
0.79046630859375, 0.782004064008128
eval metrics, batch: 4096 acc, f1
0.789337158203125, 0.7783735191190163
train metrics, batch: 4096  acc, f1 
0.8509979248046875, 0.8517905792537166
eval metrics, batch: 5120 acc, f1
0.792205810546875, 0.7852389213057878
eval metrics, batch: 6144 acc, f1
0.78985595703125, 0.7760504748276311
eval metrics, batch: 7168 acc, f1
0.788665771484375, 0.7745548067845167
Epoch loss - train: tensor(0.3470, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4315, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.791595458984375, 0.7817025221366237
train metrics acc, f1 
0.8528709411621094, 0.8540219748610013
Epoch 6/8
----------
eval metrics, batch: 1024 acc, f1
0.78875732421875, 0.7959316037735849
eval metrics, batch: 2048 acc, f1
0.791046142578125, 0.7797471611927815
eval metrics, batch: 3072 acc, f1
0.790283203125, 0.7769265727455691
eval metrics, batch: 4096 acc, f1
0.789520263671875, 0.7788785226507646
train metrics, batch: 4096  acc, f1 
0.8545188903808594, 0.8557683943800465
eval metrics, batch: 5120 acc, f1
0.79132080078125, 0.7805519897304236
eval metrics, batch: 6144 acc, f1
0.79803466796875, 0.7904237127113813
eval metrics, batch: 7168 acc, f1
0.792144775390625, 0.7809052015311867
Epoch loss - train: tensor(0.3396, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4256, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.7957763671875, 0.7838501291989665
train metrics acc, f1 
0.857086181640625, 0.857116704805492
Epoch 7/8
----------
eval metrics, batch: 1024 acc, f1
0.794677734375, 0.8004034650528065
eval metrics, batch: 2048 acc, f1
0.792022705078125, 0.7737008135480657
eval metrics, batch: 3072 acc, f1
0.794769287109375, 0.7901257684985801
eval metrics, batch: 4096 acc, f1
0.794586181640625, 0.7818223072185667
train metrics, batch: 4096  acc, f1 
0.8591995239257812, 0.8589088851852418
eval metrics, batch: 5120 acc, f1
0.79541015625, 0.7839927825750741
eval metrics, batch: 6144 acc, f1
0.7939453125, 0.7807792207792208
eval metrics, batch: 7168 acc, f1
0.80023193359375, 0.7942415288866537
Epoch loss - train: tensor(0.3320, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4321, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.79364013671875, 0.7799257957430189
train metrics acc, f1 
0.8617897033691406, 0.8612184704958535
Epoch 8/8
----------
eval metrics, batch: 1024 acc, f1
0.79290771484375, 0.7983837423495157
eval metrics, batch: 2048 acc, f1
0.796051025390625, 0.7817653397772916
eval metrics, batch: 3072 acc, f1
0.797393798828125, 0.7894853663950281
eval metrics, batch: 4096 acc, f1
0.7933349609375, 0.7809123261080556
train metrics, batch: 4096  acc, f1 
0.8627281188964844, 0.8631754251884974
eval metrics, batch: 5120 acc, f1
0.796112060546875, 0.7848932676518884
eval metrics, batch: 6144 acc, f1
0.792144775390625, 0.7754442649434572
eval metrics, batch: 7168 acc, f1
0.796905517578125, 0.7838934892027927
Epoch loss - train: tensor(0.3263, device='cuda:0', grad_fn=<DivBackward0>)
Epoch loss - valid: tensor(0.4304, device='cuda:0')
epoch end metrics
eval metrics acc, f1 
0.794464111328125, 0.7824401589301289
train metrics acc, f1 
0.8649787902832031, 0.8657138846417962
Training time 85m 47s
train_acc
0.6578750610351562	0.8055343627929688	0.8211212158203125	0.8280792236328125	0.8340263366699219	0.8379669189453125	0.8420448303222656	0.8455238342285156	0.8484382629394531	0.8509979248046875	0.8528709411621094	0.8545188903808594	0.857086181640625	0.8591995239257812	0.8617897033691406	0.8627281188964844	0.8649787902832031
train_f1
0.689145067483727	0.805558098376663	0.8209901050574151	0.8270168193018953	0.8351800711420226	0.8368328454759875	0.8416188862411499	0.8467207437043655	0.8486409264938379	0.8517905792537166	0.8540219748610013	0.8557683943800465	0.857116704805492	0.8589088851852418	0.8612184704958535	0.8631754251884974	0.8657138846417962
train_loss
tensor(0.4409, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3870, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3688, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3566, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3470, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3396, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3320, device='cuda:0', grad_fn=<DivBackward0>)	tensor(0.3263, device='cuda:0', grad_fn=<DivBackward0>)
valid_acc
0.66485595703125	0.771759033203125	0.777191162109375	0.781219482421875	0.781341552734375	0.7808837890625	0.781585693359375	0.783203125	0.7818603515625	0.78369140625	0.782958984375	0.77764892578125	0.7799072265625	0.783294677734375	0.783355712890625	0.783172607421875	0.78265380859375	0.78515625	0.78070068359375	0.783721923828125	0.781005859375	0.7867431640625	0.78582763671875	0.787933349609375	0.787506103515625	0.787384033203125	0.783905029296875	0.784332275390625	0.7867431640625	0.78863525390625	0.786834716796875	0.7891845703125	0.787841796875	0.791900634765625	0.7908935546875	0.79046630859375	0.789337158203125	0.792205810546875	0.78985595703125	0.788665771484375	0.791595458984375	0.78875732421875	0.791046142578125	0.790283203125	0.789520263671875	0.79132080078125	0.79803466796875	0.792144775390625	0.7957763671875	0.794677734375	0.792022705078125	0.794769287109375	0.794586181640625	0.79541015625	0.7939453125	0.80023193359375	0.79364013671875	0.79290771484375	0.796051025390625	0.797393798828125	0.7933349609375	0.796112060546875	0.792144775390625	0.796905517578125	0.794464111328125
valid_f1
0.6916381198405122	0.7784722016527946	0.7744028674721132	0.777367162510481	0.7730943408176838	0.7733871985860371	0.7754525774166222	0.7729771187523968	0.7717169136433316	0.786570310147546	0.7735752944922	0.7581972653657242	0.7667227325656618	0.7709871964395136	0.7729700342191947	0.7767057418523524	0.7727939769029541	0.7922323220399008	0.7709422414892261	0.7688443850092957	0.7668767461503476	0.7773955147808359	0.7749486916367367	0.7831486971446403	0.7757704569606801	0.7948166691208953	0.7692357829558416	0.7673109216028449	0.7765413149143003	0.7736157416486893	0.7727494550541693	0.7709549071618037	0.7759009735026755	0.7937573722892659	0.7876534027519524	0.782004064008128	0.7783735191190163	0.7852389213057878	0.7760504748276311	0.7745548067845167	0.7817025221366237	0.7959316037735849	0.7797471611927815	0.7769265727455691	0.7788785226507646	0.7805519897304236	0.7904237127113813	0.7809052015311867	0.7838501291989665	0.8004034650528065	0.7737008135480657	0.7901257684985801	0.7818223072185667	0.7839927825750741	0.7807792207792208	0.7942415288866537	0.7799257957430189	0.7983837423495157	0.7817653397772916	0.7894853663950281	0.7809123261080556	0.7848932676518884	0.7754442649434572	0.7838934892027927	0.7824401589301289
valid_loss
tensor(0.4451, device='cuda:0')	tensor(0.4436, device='cuda:0')	tensor(0.4324, device='cuda:0')	tensor(0.4348, device='cuda:0')	tensor(0.4315, device='cuda:0')	tensor(0.4256, device='cuda:0')	tensor(0.4321, device='cuda:0')	tensor(0.4304, device='cuda:0')
Experiment end
########################################